{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Using Centroid Clustering on your own data!\n",
        "subtitle: AgroGeo24 Clustering Workshop Part 6\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjkND_j_FnFy",
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "**Please save a copy of this notebook in your Google Drive before running**\n",
        "\n",
        "Click \"File\" and \"Save a copy in Drive\" to save a copy of this notebook to your own Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "This part of the workshop allows participants to try clustering on their own data, if they have brought some.\n",
        "\n",
        "It should be a csv file, with format:\n",
        "\n",
        "```\n",
        "Column 1.   X Coordinate (X)\n",
        "Column 2.   Y Coordinate (Y)\n",
        "Column 3.   Data Layer 1\n",
        ".\n",
        ".\n",
        ".\n",
        "Column N.   Data Layer N\n",
        "```\n",
        "\n",
        "The aim of this workshop is to determine the appropriate number of clusters for your data using KMeans clustering. We will compare some tradition methods with a recently proposed MCASD method.\n",
        "\n",
        "No coding experience is required to run this code. All the code contains comments describing what each line does. Please click \"Show Code\" on any section to view the code.\n",
        "\n",
        "Please feel free to ask questions if you don't understand any parts.\n",
        "\n",
        "# Section 0\n",
        "\n",
        "This section sets up the Python environment for the clustering analysis.\n",
        "\n",
        "It imports essential libraries such as Pandas for data manipulation, NumPy for numerical operations, Matplotlib for data visualization, scikit-learn for machine learning tools, and other supporting libraries.\n",
        "\n",
        "Additionally, it configures the display.\n",
        "\n",
        "The code also imports specific functions and modules required for the clustering analysis, such as KMeans.\n",
        "\n",
        "Finally, it sets up tools for working with images, zip files, and file uploads in Google Colab. This preparation ensures that the subsequent code can efficiently perform clustering analysis and handle related tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "\n",
        "\n",
        "Press the play button below:\n",
        "\n",
        "# ⬇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7dMOY8dsFrIU"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import imageio.v2 as imageio\n",
        "import matplotlib\n",
        "import imageio.v2 as imageio\n",
        "import os\n",
        "import re\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from PIL import Image\n",
        "from zipfile import ZipFile\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# Set display options to show all rows and columns\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FFyzLdvmiue"
      },
      "source": [
        "# Section 1\n",
        "\n",
        "Now we are going to read in the data from your local device.\n",
        "\n",
        "The Data should be in CSV format with the following header descriptors:\n",
        "\n",
        "1.   Column 1 = X Coordinate\n",
        "2.   Column 2 = Y Coordinate\n",
        "3.   Columns 3..N = Data\n",
        "\n",
        "Data will be rounded to 4 decimal places. (Change this in the code if necessary)\n",
        "\n",
        "Press the play button below to select the correct file from your computer.\n",
        "\n",
        "You will see a button \"Choose files\". Click this and navigate to where this file is stored on your machine. Then highlight it and click \"open\".\n",
        "\n",
        "You will see a progress message as the file is uploaded to this Google Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VXbCAhKcmatl",
        "tags": [
          "remove-output"
        ]
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#### Section 1: Import CSV file  ####\n",
        "\n",
        "# Prompt the user to upload a file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file name\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Extract the filename without the extension\n",
        "file_name_without_extension = os.path.splitext(file_name)[0]\n",
        "\n",
        "# Remove numerical suffixes from the filename\n",
        "file_name_without_extension = re.sub(r'\\(\\d+\\)', '', file_name_without_extension)\n",
        "file_name_without_extension = re.sub(r'\\(\\ \\)', '', file_name_without_extension)\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(file_name).round(4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJd39JCjqVg7"
      },
      "source": [
        "Excellent. You have now loaded the CSV file into the Google Colab environment and are ready to take a look at the data.\n",
        "\n",
        "# Section 2\n",
        "\n",
        "In this section, we're inspecting and refining the dataset. Initially, we showcase a snippet of the original dataset, including its structure and the number of rows.\n",
        "\n",
        "Next, we perform data cleaning by eliminating rows containing NaN (Not a Number) and blank values. The cleaned dataset is then displayed, again with a snippet and the updated row count.\n",
        "\n",
        "This process ensures that the dataset is well-prepared for subsequent analyses by removing any instances of missing or empty data.\n",
        "\n",
        "Note that this dataset contained no blank values, so the number of rows doesn't change"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "\n",
        "Press the play button to run the code\n",
        "\n",
        "# ⬇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "50ol94wvqX9Z"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#### Section 2: Data Cleaning ####\n",
        "\n",
        "# Display the original DataFrame and its shape\n",
        "print(\"Original Data:\")\n",
        "print(df.head(5).to_string(index=False))\n",
        "print(f\"Number of rows before cleaning: {df.shape[0]}\")\n",
        "\n",
        "# Remove rows with NaN and blank values\n",
        "df_cleaned = df.dropna().replace('', np.nan).dropna()\n",
        "\n",
        "# Display the cleaned DataFrame and its shape\n",
        "print(\"\\nData after removing NaN and blank values:\")\n",
        "print(df_cleaned.head(5).to_string(index=False))\n",
        "print(f\"Number of rows after cleaning: {df_cleaned.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7f8gn1aI6-a"
      },
      "source": [
        "# Section 3\n",
        "\n",
        "Building on the cleaned dataset from the previous section, we now visualize the data columns through scatter plots.\n",
        "\n",
        "Each subplot represents a specific data column, showcasing its spatial distribution across the X and Y coordinates. The color intensity in each plot reflects the values of the corresponding data column.\n",
        "\n",
        "The number of rows and columns for the subplot grid is dynamically calculated based on the available data columns.\n",
        "\n",
        "This visualization provides an initial exploration of how different data layers are distributed in geographical space, setting the stage for further analysis and insights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "\n",
        "Press the play button to run the code\n",
        "\n",
        "# ⬇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V4hI29kPI-JY"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#### Section 3 Data Viewing ###\n",
        "\n",
        "# Get the list of non-coordinate column names\n",
        "data_column_names = df_cleaned.columns[2:]\n",
        "\n",
        "# Get the list of non-coordinate column names\n",
        "data_column_names = df_cleaned.columns[2:]\n",
        "\n",
        "# Calculate the number of rows and columns for subplots\n",
        "num_plots = len(data_column_names)\n",
        "num_plots_per_row = 3\n",
        "num_rows = (num_plots + num_plots_per_row - 1) // num_plots_per_row\n",
        "num_cols = min(num_plots, num_plots_per_row)\n",
        "\n",
        "# Create subplots with the specified number of rows and columns\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5))\n",
        "\n",
        "# Flatten the axes to simplify indexing\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Loop through each data column and create scatter plots\n",
        "for i, column_name in enumerate(data_column_names):\n",
        "    # Extract the data for the current column\n",
        "    column_data = df_cleaned[column_name]\n",
        "\n",
        "    # Calculate the position in the subplot grid\n",
        "    row_index = i // num_cols\n",
        "    col_index = i % num_cols\n",
        "\n",
        "    # Create a scatter plot\n",
        "    scatter = axes[i].scatter(df_cleaned['X'], df_cleaned['Y'], c=column_data, cmap='viridis', marker='o', s=10)\n",
        "\n",
        "    # Set plot properties\n",
        "    axes[i].set_title(f'Scatter Plot for {column_name}')\n",
        "    axes[i].set_xlabel('X Coordinate')\n",
        "    axes[i].set_ylabel('Y Coordinate')\n",
        "\n",
        "    # Set the number of tick marks on the X and Y axes\n",
        "    axes[i].xaxis.set_major_locator(MaxNLocator(nbins=3))\n",
        "    axes[i].yaxis.set_major_locator(MaxNLocator(nbins=3))\n",
        "\n",
        "    # Add a color bar scaled to the min and max of the current column\n",
        "    cbar = plt.colorbar(scatter, ax=axes[i])\n",
        "    cbar.set_label(column_name)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fclHVETJsEgL"
      },
      "source": [
        "# Section 4\n",
        "\n",
        "In this section, we strategically divide our dataset into two key components: coordinates and remaining data. The first two columns, containing coordinate information, are isolated to construct the \"Coordinates\" DataFrame.\n",
        "\n",
        "Simultaneously, the remaining data, excluding the initial two columns, forms the \"Remaining Data\" DataFrame.\n",
        "\n",
        "This separation serves a pivotal purpose—clustering analysis will solely operate on the remaining data. Subsequently, the cluster labels obtained can be associated with their respective X and Y coordinates. This distinction is fundamental for generating geographical cluster maps, allowing us to visually interpret and understand the spatial distribution of clusters across the dataset.\n",
        "\n",
        "This section also focuses on data normalization, a crucial step before applying clustering algorithms.\n",
        "\n",
        "**You can choose the normalisation type here, column wise, or dataset wise.**\n",
        "\n",
        "The resulting normalized data is displayed, providing an insight into the standardized values across the dataset.\n",
        "\n",
        "Normalization enhances the accuracy of clustering algorithms, ensuring that features with different scales contribute equally to the clustering process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "\n",
        "Press the play button to run the code\n",
        "\n",
        "# ⬇\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2N_cvN9IsHdD"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#### Section 4 Separate the Data for clustering ####\n",
        "\n",
        "# Extract coordinate information (assuming it's in the first two columns)\n",
        "coordinates = df_cleaned.iloc[:, :2]\n",
        "\n",
        "# Extract the remaining data (excluding the first two columns)\n",
        "remaining_data = df_cleaned.iloc[:, 2:]\n",
        "remaining_data = remaining_data.round(2)\n",
        "\n",
        "# Display the Coordinates DataFrame\n",
        "print(\"\\nCoordinates:\")\n",
        "print(coordinates.head(5).to_string(index=False))  # Use to_string to prevent truncation\n",
        "print(f\"Number of rows: {df_cleaned.shape[0]}\")\n",
        "\n",
        "# Display the Remaining Data DataFrame\n",
        "print(\"\\nRemaining Data:\")\n",
        "print(remaining_data.head(5).to_string(index=False))  # Use to_string to prevent truncation\n",
        "print(f\"Number of rows: {df_cleaned.shape[0]}\")\n",
        "\n",
        "# Prompt the user to choose the maximum number of clusters for the Elbow Method\n",
        "norm_type = int(input(\"Select normalization type. 1 = Column-wise normalization. 2 = Dataset-wise normalization: \"))\n",
        "\n",
        "if norm_type == 1:\n",
        "    # Custom normalization using MinMaxScaler\n",
        "    min_vals = remaining_data.min()\n",
        "    max_vals = remaining_data.max()\n",
        "\n",
        "    normalized_data = (remaining_data - min_vals) / (max_vals - min_vals)\n",
        "elif norm_type == 2:\n",
        "    # Custom normalization using MinMaxScaler\n",
        "    min_vals = remaining_data.min().min()\n",
        "    max_vals = remaining_data.max().max()\n",
        "\n",
        "    normalized_data = (remaining_data - min_vals) / (max_vals - min_vals)\n",
        "else:\n",
        "    print(\"Invalid norm_type. Please use 1 or 2.\")\n",
        "\n",
        "# Round the normalized data to 4 decimal places\n",
        "normalized_data = normalized_data.round(4)\n",
        "\n",
        "# Convert the rounded normalized data back to a DataFrame and set column names\n",
        "normalized_df = pd.DataFrame(normalized_data, columns=remaining_data.columns)\n",
        "\n",
        "# Display the normalized data\n",
        "print(\"\\nNormalized Data:\")\n",
        "print(normalized_data.head(5).to_string(index=False))  # Use to_string to prevent truncation\n",
        "print(f\"Number of rows: {df_cleaned.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gPlwgyJRFoo"
      },
      "source": [
        "# Section 5\n",
        "\n",
        "This section guides the user in determining the optimal number of clusters (k) for the K-Means algorithm by utilizing both the Elbow Method and Silhouette Scores.\n",
        "\n",
        "After specifying the maximum number of clusters to consider, the code calculates the Within-Cluster Sum of Squares (WCSS) distance using the Elbow Method. The Elbow Method graph illustrates the trade-off between clustering complexity and WCSS reduction, helping identify an optimal k value.\n",
        "\n",
        "Simultaneously, Silhouette Scores, a measure of how well-separated clusters are, are computed and presented on the same graph. Silhouette Scores range from -1 to 1, where higher scores indicate better-defined clusters.\n",
        "\n",
        "When assessing the graph, users should look for the \"elbow\" point where WCSS plateaus, suggesting diminishing returns with additional clusters.\n",
        "\n",
        "Additionally, a high Silhouette Score at the \"elbow\" reinforces the choice, ensuring a balance between compact clusters and distinct cluster boundaries for effective clustering\n",
        "\n",
        "When running the cell, you will be prompted to enter the max number of clusters. (i.e., 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "\n",
        "Press the play button to run the code.\n",
        "\n",
        "# ⬇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MtGPYPc3RElB"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#### Section 6 Elbow Method with Silhouette Scores ####\n",
        "\n",
        "# Prompt the user to choose the maximum number of clusters for the Elbow Method\n",
        "max_clusters_elbow = int(input(\"Enter the maximum number of clusters for the Elbow Method: \"))\n",
        "\n",
        "# Calculate the within-cluster sum of squares (WCSS) and Silhouette Scores for different values of k\n",
        "wcss = []\n",
        "silhouette = []\n",
        "for k in range(2, max_clusters_elbow + 1):\n",
        "    kmeans = KMeans(n_clusters=k, n_init=1, init='k-means++')\n",
        "    kmeans.fit(normalized_data)\n",
        "    sscore = round(silhouette_score(normalized_data, kmeans.labels_), 2)\n",
        "    silhouette.append(sscore)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow Method graph with Silhouette Scores as a bar graph\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot WCSS on the left y-axis\n",
        "ax1.plot(range(2, max_clusters_elbow + 1), wcss, marker='o', color='blue', label='WCSS')\n",
        "ax1.set_xlabel('Number of Clusters (k)')\n",
        "ax1.set_ylabel('Within-Cluster Sum of Squares (WCSS)', color='blue')\n",
        "\n",
        "# Set the x-axis ticks to show only integers\n",
        "plt.xticks(range(2, max_clusters_elbow + 1))\n",
        "\n",
        "# Create a second y-axis for Silhouette Scores\n",
        "ax2 = ax1.twinx()\n",
        "ax2.bar(range(2, max_clusters_elbow + 1), silhouette, color='green', alpha=0.5, label='Silhouette Scores')\n",
        "ax2.set_ylabel('Silhouette Scores', color='green')\n",
        "\n",
        "# Add legend\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax2.legend(lines + lines2, labels + labels2, loc='upper right')\n",
        "\n",
        "plt.title('Elbow Method with Silhouette Scores for Optimal k')\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swOYqT22-NzN"
      },
      "source": [
        "# Section 6\n",
        "\n",
        "In this section, users will perform K-Means clustering on the preprocessed EMI data.\n",
        "\n",
        "The optimal number of clusters (k) can be determined by referencing the results from the previous Elbow Method and Silhouette Scores analysis (Section 5). After entering the desired number of clusters, the code applies K-Means clustering and displays key information.\n",
        "\n",
        "The results include a count of occurrences for each cluster label via a plot of the classified image and \"spectral\" graph showing the cluster centers.\n",
        "\n",
        "The scatter plot provides an overview of spatial distribution of the clusters, while the line plot illustrates how cluster center values vary across the survey.\n",
        "\n",
        "You will be prompted to enter the number of clusters, which should be based on the Elbow and Silhouette results, but feel free to run this code a few times for various number of clusters and take a look at the results!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "\n",
        "Press the play button to run the code.\n",
        "\n",
        "# ⬇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UsozNi-Z-Q4w"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "##### Section 6 K Means Clustering ####\n",
        "\n",
        "### Section 6.1 Perform K Means Clustering ###\n",
        "\n",
        "# Prompt the user to choose the number of clusters for K-Means\n",
        "num_clusters = int(input(\"Enter the number of clusters for K-Means: \"))\n",
        "\n",
        "# Initialize the K-Means model\n",
        "kmeans = KMeans(n_clusters=num_clusters, init = 'k-means++', n_init=1)\n",
        "\n",
        "# Fit the K-Means model to the normalized data\n",
        "kmeans.fit(normalized_data)\n",
        "\n",
        "# Get the cluster labels for each data point\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "# Get the cluster centers\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "### Section 6.2 Sort the Cluster centers ###\n",
        "\n",
        "# Calculate the distances of cluster centers from the origin (0, 0)\n",
        "distances_from_origin = np.sqrt(np.sum(cluster_centers ** 2, axis=1))\n",
        "\n",
        "# Sort cluster centers based on distances from the origin\n",
        "sorted_indices = np.argsort(distances_from_origin)\n",
        "\n",
        "# Sort cluster centers and labels\n",
        "sorted_cluster_centers = cluster_centers[sorted_indices]\n",
        "sorted_cluster_labels = np.zeros_like(cluster_labels)\n",
        "\n",
        "# Relabel the cluster labels based on the sorted order\n",
        "for new_label, old_label in enumerate(sorted_indices):\n",
        "    sorted_cluster_labels[cluster_labels == old_label] = new_label\n",
        "\n",
        "# Calculate the count of each cluster label\n",
        "cluster_labels_count = dict(zip(*np.unique(sorted_cluster_labels, return_counts=True)))\n",
        "\n",
        "### Section 6.3 Denormalize the cluster centers ###\n",
        "\n",
        "# Denormalize the data using the inverse transformation\n",
        "if norm_type == 1:\n",
        "    cluster_centers_original_scale = sorted_cluster_centers * (max_vals.values - min_vals.values) + min_vals.values\n",
        "elif norm_type == 2:\n",
        "    cluster_centers_original_scale = sorted_cluster_centers * (max_vals - min_vals) + min_vals\n",
        "\n",
        "### Section 6.4 Display Clustering counts for visual QC ###\n",
        "\n",
        "# Display the count of each cluster label\n",
        "print(\"\\nCount of Each Cluster Label:\")\n",
        "for label, count in cluster_labels_count.items():\n",
        "    print(f\"Cluster {label}: {count} occurrences\")\n",
        "\n",
        "### Section 6.5 Save results to CSV ###\n",
        "\n",
        "# Create a DataFrame with X, Y, Cluster, and Remaining Data\n",
        "clustered_data_df = pd.DataFrame({\n",
        "    'X': coordinates['X'],\n",
        "    'Y': coordinates['Y'],\n",
        "    'Cluster Number': sorted_cluster_labels,\n",
        "    **{f'{col}': remaining_data[col] for col in remaining_data.columns}\n",
        "})\n",
        "clustered_data_df = clustered_data_df.round(4)\n",
        "\n",
        "# Create a DataFrame with Cluster center data\n",
        "center_data_df = pd.DataFrame(cluster_centers_original_scale, columns=remaining_data.columns)\n",
        "\n",
        "# Add a new column 'Cluster Number' to indicate the cluster number for each row\n",
        "center_data_df.insert(0, 'Cluster Number', range(num_clusters))\n",
        "center_data_df = center_data_df.round(4)\n",
        "\n",
        "### Section 6.6 Plot KMeans Clustering results ###\n",
        "\n",
        "# Create a scatter plot of X, Y, and final cluster label\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Scatter Plot with Cluster Labels\n",
        "scatter = ax1.scatter(coordinates['X'], coordinates['Y'], c=sorted_cluster_labels, cmap='viridis', marker='o', s=30)\n",
        "\n",
        "# Set plot properties for Plot 1\n",
        "ax1.set_title('Scatter Plot with Cluster Labels')\n",
        "ax1.set_xlabel('X Coordinate')\n",
        "ax1.set_ylabel('Y Coordinate')\n",
        "\n",
        "# Create a discrete color map with the number of clusters for Plot 1\n",
        "cmap_discrete = matplotlib.colormaps.get_cmap('viridis')\n",
        "\n",
        "# Define boundaries for the discrete color map for Plot 1\n",
        "boundaries = np.arange(-0.5, num_clusters, 1)\n",
        "\n",
        "# Create a BoundaryNorm for the color map for Plot 1\n",
        "norm_discrete = mcolors.BoundaryNorm(boundaries, cmap_discrete.N, clip=True)\n",
        "\n",
        "# Add a discrete color bar with integer cluster labels for Plot 1\n",
        "cbar = plt.colorbar(scatter, ax=ax1, ticks=np.arange(num_clusters), cmap=cmap_discrete, norm=norm_discrete, boundaries=boundaries)\n",
        "cbar.set_label('Cluster Label')\n",
        "\n",
        "# Set the number of tick marks on the X and Y axes for Plot 1\n",
        "ax1.xaxis.set_major_locator(MaxNLocator(nbins=3))\n",
        "ax1.yaxis.set_major_locator(MaxNLocator(nbins=3))\n",
        "\n",
        "# Plot 2: Line Plot of Unnormalized Cluster Centers\n",
        "for cluster_label in range(num_clusters):\n",
        "    color = cmap_discrete(cluster_label / (num_clusters - 1))  # Match color from scatter plot\n",
        "    ax2.plot(remaining_data.columns, cluster_centers_original_scale[cluster_label], label=f'Cluster {cluster_label}', color=color)\n",
        "    ax2.set_ylim(remaining_data.min().min(), remaining_data.max().max())\n",
        "\n",
        "# Set plot properties for Plot 2\n",
        "ax2.set_title('Line Plot of Unnormalized Cluster Centers')\n",
        "ax2.set_xlabel('Column Name')\n",
        "ax2.set_ylabel('Cluster Center Values)')\n",
        "ax2.legend()\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIms5Ei5t6Aw"
      },
      "source": [
        "# Section 7\n",
        "\n",
        "This section introduces the MCASD (Multiple Cluster Average Standard Deviation) method, designed to aid participants in identifying the optimal number of clusters for their dataset.\n",
        "\n",
        "MCASD was first published as a method by [O'Leary et al 2023](https://doi.org/10.1016/j.geoderma.2023.116348).\n",
        "\n",
        "MCASD evaluates the stability of cluster centers across multiple attempts and cluster numbers. Participants will input the maximum number of clusters and the maximum number of attempts per cluster.\n",
        "\n",
        "The code loops through different cluster numbers, applying K-Means clustering multiple times to analyze stability.\n",
        "\n",
        "The results include GIFs illustrating scatter plots and line plots for each attempt. Additionally, MCASD metrics are calculated, providing insights into the stability and consistency of clusters. A line plot visualizes the MCASD metric across various cluster numbers, aiding participants in selecting the optimal cluster count.\n",
        "\n",
        "**All results, including plots and metrics, are compressed into a zip file for easy download.**\n",
        "\n",
        "Please save this ZIP file to your computer.\n",
        "\n",
        "\n",
        "The ultimate goal is to assist participants in making informed decisions about the optimal number of clusters for their specific dataset.\n",
        "\n",
        "When running the cell, you will be prompted to enter the number of max number of clusters (i.e., 10) and max attempts (i.e., 10).\n",
        "\n",
        "Note this might take some time depending on the max number of clusters and max attempts chosen.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "\n",
        "Press the play button to run the code.\n",
        "# ⬇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "t88wlCWxt9Af"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#### Section 7 MCASD Method ####\n",
        "### Section 7.1 Get information from the user ###\n",
        "\n",
        "# Prompt the user for the maximum number of clusters for MCASD Method\n",
        "max_num_clusters = int(input(\"Enter the maximum number of clusters for MCASD Method: \"))\n",
        "# Prompt the user for the maximum number of attempts for MCASD Method\n",
        "max_attempts = int(input(\"Enter the maximum number of attempts for MCASD Method: \"))\n",
        "\n",
        "### Section 7.2 Loop for MCASD Method ###\n",
        "# Create a DataFrame to store MCASD Metrics\n",
        "mcasd_metrics_df = pd.DataFrame(index=['MCASD Metric', 'MCASD Error'], columns=range(1, max_num_clusters + 1))\n",
        "\n",
        "print(f\"\\nCalculating MCASD Metrics...\")\n",
        "\n",
        "# Create a zip file to store all results\n",
        "zip_filename = f'AgroGeo24_WS_Part_3_kmeans_plots.zip'\n",
        "with ZipFile(zip_filename, 'w') as zip_file:\n",
        "\n",
        "    # Loop through the various number of clusters\n",
        "    for num_clusters in range(1, max_num_clusters + 1):\n",
        "        images_attempt = []  # List to store images for the current attempt\n",
        "        distances_df = pd.DataFrame()  # Initialize distances DataFrame\n",
        "\n",
        "        # Cluster the data a user specified number of times (Attempts)\n",
        "        for attempt in range(1, max_attempts + 1):\n",
        "            #print(f\"\\nNumber of Clusters: {num_clusters}: Attempt {attempt} of {max_attempts}\")\n",
        "\n",
        "            # Initialize the K-Means model\n",
        "            kmeans = KMeans(n_clusters=num_clusters, n_init=1, init='k-means++')\n",
        "\n",
        "            # Fit the K-Means model to the normalized data\n",
        "            kmeans.fit(normalized_data)\n",
        "\n",
        "            # Get the cluster labels for each data point\n",
        "            cluster_labels = kmeans.labels_\n",
        "\n",
        "            # Get the cluster centers\n",
        "            cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "            ### Section 7.2.1 Sort the Cluster centers ###\n",
        "\n",
        "            # Calculate the distances of cluster centers from the origin (0, 0)\n",
        "            distances_from_origin = np.sqrt(np.sum(cluster_centers ** 2, axis=1))\n",
        "\n",
        "            # Sort cluster centers based on distances from the origin\n",
        "            sorted_indices = np.argsort(distances_from_origin)\n",
        "\n",
        "            # Sort cluster centers and labels\n",
        "            sorted_cluster_centers = cluster_centers[sorted_indices]\n",
        "            sorted_cluster_labels = np.zeros_like(cluster_labels)\n",
        "\n",
        "            # Relabel the cluster labels based on the sorted order\n",
        "            for i, new_label in enumerate(np.arange(num_clusters)):\n",
        "                old_label = sorted_indices[i]\n",
        "                sorted_cluster_labels[cluster_labels == old_label] = new_label\n",
        "\n",
        "            ### Section 7.2.2 Calculate the distance (in the dataspace) between each datapoint and its closest cluster center ###\n",
        "\n",
        "            # Calculate the distances between cluster centers and data\n",
        "            distances = np.linalg.norm(normalized_data.values[:, np.newaxis, :] - cluster_centers, axis=-1)\n",
        "\n",
        "            # Get the smallest distance for each data point\n",
        "            min_distances = np.min(distances, axis=1)\n",
        "\n",
        "            # Create a DataFrame for distances with only the smallest distances\n",
        "            new_column = pd.DataFrame(min_distances, columns=[f'Attempt_{attempt}'])\n",
        "\n",
        "            # Append the new column to the existing distances_df\n",
        "            distances_df = pd.concat([distances_df, new_column], axis=1)\n",
        "\n",
        "            ### Section 7.2.3 Denormalize the cluster centers ###\n",
        "\n",
        "            # Denormalize the data using the inverse transformation\n",
        "            if norm_type == 1:\n",
        "                cluster_centers_original_scale = sorted_cluster_centers * (max_vals.values - min_vals.values) + min_vals.values\n",
        "            elif norm_type == 2:\n",
        "                cluster_centers_original_scale = sorted_cluster_centers * (max_vals - min_vals) + min_vals\n",
        "\n",
        "            ### Section 7.2.4 Create and save plots for later GIF creation ###\n",
        "\n",
        "            # Create a scatter plot of X, Y, and final cluster label\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "            # Plot 1: Scatter Plot with Cluster Labels\n",
        "            scatter = ax1.scatter(coordinates['X'], coordinates['Y'], c=sorted_cluster_labels, cmap='viridis',\n",
        "                                  marker='o', s=30)\n",
        "\n",
        "            # Set plot properties for Plot 1\n",
        "            ax1.set_title(f'Scatter Plot with Cluster Labels (Attempt {attempt}, Clusters {num_clusters})')\n",
        "            ax1.set_xlabel('X Coordinate')\n",
        "            ax1.set_ylabel('Y Coordinate')\n",
        "\n",
        "            # Create a discrete color map with the number of clusters for Plot 1\n",
        "            cmap_discrete = matplotlib.colormaps.get_cmap('viridis')\n",
        "\n",
        "            # Define boundaries for the discrete color map for Plot 1\n",
        "            boundaries = np.arange(-0.5, num_clusters, 1)\n",
        "\n",
        "            # Create a BoundaryNorm for the color map for Plot 1\n",
        "            norm_discrete = mcolors.BoundaryNorm(boundaries, cmap_discrete.N, clip=True)\n",
        "\n",
        "            # Add a discrete color bar with integer cluster labels for Plot 1\n",
        "            cbar = plt.colorbar(scatter, ax=ax1, ticks=np.arange(num_clusters), cmap=cmap_discrete, norm=norm_discrete,\n",
        "                                boundaries=boundaries)\n",
        "            cbar.set_label('Cluster Label')\n",
        "\n",
        "            # Set the number of tick marks on the X and Y axes for Plot 1\n",
        "            ax1.xaxis.set_major_locator(MaxNLocator(nbins=3))\n",
        "            ax1.yaxis.set_major_locator(MaxNLocator(nbins=3))\n",
        "\n",
        "            # Plot 2: Line Plot of Unnormalized Cluster Centers\n",
        "            for cluster_label in range(num_clusters):\n",
        "                color = cmap_discrete(cluster_label / (num_clusters))  # Match color from scatter plot\n",
        "                ax2.plot(remaining_data.columns, cluster_centers_original_scale[cluster_label],\n",
        "                        label=f'Cluster {cluster_label}', color=color)\n",
        "                ax2.set_ylim(remaining_data.min().min(), remaining_data.max().max())\n",
        "\n",
        "            # Set plot properties for Plot 2\n",
        "            ax2.set_title(f'Line Plot of Cluster Centers (Attempt {attempt}, Clusters {num_clusters})')\n",
        "            ax2.set_xlabel('Column Name')\n",
        "            ax2.set_ylabel('Cluster Center Value')\n",
        "            ax2.legend()\n",
        "\n",
        "            # Save the plots\n",
        "            plot_filename = f'kmeans_plots_Attempt_{attempt}_Num_Clusters_{num_clusters}.png'\n",
        "            plot_filepath = os.path.join(plot_filename)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(plot_filepath)\n",
        "            #plt.show()  # Display the plot\n",
        "            images_attempt.append(plot_filepath)  # Append the plot to the list\n",
        "            plt.close()\n",
        "\n",
        "        # Convert the images for the current number of clusters to a GIF\n",
        "        gif_filename = f'kmeans_plots_Num_Clusters_{num_clusters}.gif'\n",
        "        with imageio.get_writer(gif_filename, mode='I', fps=1, loop=0) as writer_attempt:\n",
        "            for image_filename in images_attempt:\n",
        "                # Adjust the image filename to include the subfolder\n",
        "                image = imageio.imread(image_filename)\n",
        "                writer_attempt.append_data(image)\n",
        "\n",
        "                # Remove individual plot files after adding to GIF\n",
        "                os.remove(image_filename)\n",
        "\n",
        "        # Save the GIF to the current cluster folder\n",
        "        zip_file.write(gif_filename)\n",
        "\n",
        "        # Remove the GIF file after adding to the zip file\n",
        "        os.remove(gif_filename)\n",
        "\n",
        "        ### Section 7.3 Calculate MCASD metrics ###\n",
        "\n",
        "        # Calculate Standard Deviation along each row\n",
        "        row_std_dev = distances_df.std(axis=1)\n",
        "\n",
        "        # Calculate Average of Standard Deviation for all Rows\n",
        "        avg_std_dev = row_std_dev.mean()\n",
        "\n",
        "        # Calculate Standard Deviation of the first Standard Deviation for all rows\n",
        "        error = row_std_dev.std(axis=0)\n",
        "\n",
        "        # Save values in the mcasd_metrics_df DataFrame\n",
        "        mcasd_metrics_df.at['MCASD Metric', num_clusters] = avg_std_dev\n",
        "        mcasd_metrics_df.at['MCASD Error', num_clusters] = error\n",
        "\n",
        "        ### Section 7.4 Save results for Cluster number to a CSV ###\n",
        "\n",
        "        # Output file names\n",
        "        output_cluster_filename = f'{file_name_without_extension}_kmeans_{num_clusters}_cluster_data.csv'\n",
        "        output_center_filename = f'{file_name_without_extension}_kmeans_{num_clusters}_cluster_centers.csv'\n",
        "\n",
        "        # Create a DataFrame with X, Y, Cluster, and Remaining Data\n",
        "        clustered_data_df = pd.DataFrame({\n",
        "            'X': coordinates['X'],\n",
        "            'Y': coordinates['Y'],\n",
        "            'Cluster Number': sorted_cluster_labels,\n",
        "            'MCASD Metric': row_std_dev,\n",
        "            **{f'{col}': remaining_data[col] for col in remaining_data.columns}\n",
        "        })\n",
        "        clustered_data_df = clustered_data_df.round(4)\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        clustered_data_df.to_csv(output_cluster_filename, index=False)\n",
        "\n",
        "        # Create a DataFrame with Cluster center data\n",
        "        center_data_df = pd.DataFrame(cluster_centers_original_scale, columns=remaining_data.columns)\n",
        "\n",
        "        # Add a new column 'Cluster Number' to indicate the cluster number for each row\n",
        "        center_data_df.insert(0, 'Cluster Number', range(num_clusters))\n",
        "        center_data_df = center_data_df.round(4)\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        center_data_df.to_csv(output_center_filename, index=False)\n",
        "\n",
        "        # Save the csv to the current cluster folder\n",
        "        zip_file.write(output_cluster_filename)\n",
        "        zip_file.write(output_center_filename)\n",
        "\n",
        "        # Remove the csv file after adding to the zip file\n",
        "        os.remove(output_cluster_filename)\n",
        "        os.remove(output_center_filename)\n",
        "\n",
        "    # Save mcasd_metrics_df to a CSV file\n",
        "    mcasd_metrics_csv_filename = 'mcasd_metrics.csv'\n",
        "    mcasd_metrics_df.to_csv(mcasd_metrics_csv_filename)\n",
        "\n",
        "    # Add mcasd_metrics CSV file to the zip file\n",
        "    zip_file.write(mcasd_metrics_csv_filename)\n",
        "\n",
        "    # Remove the mcasd_metrics CSV file after adding to the zip file\n",
        "    os.remove(mcasd_metrics_csv_filename)\n",
        "\n",
        "    ### Section 7.5 Create MCASD metric plot for visual QC ###\n",
        "\n",
        "    # Make a 2D Line plot\n",
        "    plt.errorbar(mcasd_metrics_df.columns, mcasd_metrics_df.loc['MCASD Metric'],\n",
        "                yerr=mcasd_metrics_df.loc['MCASD Error'], xerr=0, fmt='-o', capsize=5, ecolor='red', errorevery=1,\n",
        "                elinewidth=0.8)\n",
        "    plt.xlabel('Cluster Number')\n",
        "    plt.ylabel('MCASD Stability')\n",
        "    plt.title('MCASD Metric vs Cluster Number')\n",
        "    plt.ylim(0)  # Set Y Axis starting at 0\n",
        "    plt.xticks(np.arange(1, max_num_clusters + 1, 1))  # Set X Tick marks at all integers\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Save the 2D line plot to the zip file\n",
        "    line_plot_filename = 'mcasd_line_plot.png'\n",
        "    plt.savefig(line_plot_filename)\n",
        "    zip_file.write(line_plot_filename)\n",
        "    os.remove(line_plot_filename)  # Remove the saved file after adding to the zip file\n",
        "\n",
        "# Inform the user that the MCASD Method clustering is complete\n",
        "print(\"\\nMCASD Method clustering complete.\")\n",
        "\n",
        "### Section 7.6 Save the final zip file to the user's local machine ###\n",
        "\n",
        "# Move the zip file to the user's local machine\n",
        "files.download(zip_filename)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
